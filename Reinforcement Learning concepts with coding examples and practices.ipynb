{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning concepts with coding examples and Practices\n",
    "### Author: Ashis Kumer Biswas\n",
    "#### Date: 10/5/2020 9:20am (MST)\n",
    "\n",
    "##Reference /reading materials\n",
    "* Reinforcement Learning -- an introduction (second edition) by Richard Sutton and Andrew Barto\n",
    "    - [Full PDF ](http://www.incompleteideas.net/book/the-book-2nd.html)\n",
    "    - [Source codes](http://www.incompleteideas.net/book/code/code2nd.html)\n",
    "    - Required reading chapters:\n",
    "        * 1 -- 13 (most)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outlines\n",
    "* [@TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Objective of Reinforcement Learning (RL)\n",
    "* It is learning what to do -- how to map situations to actions -- so to maximize a numerical reward signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* An action may affect not only the immediate reward but also the next situation/state and, through that, all subsequent rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The two characteristics are the two important features of RL: \n",
    "    * (i) trial-and-error search, (ii) delayed reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two broad categories of learning\n",
    "1. Tabular solution methods\n",
    "2. Approximate solution methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Tabular solution methods\n",
    "* This category of methods are applicable where the state and action spaces are small enough for the approximate value functions to be represented as arrays (i.e., tables).\n",
    "* In most cases, the methods can often find exact solutions, that is, they can often find exactly the optimal value function and the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Approximate solution methods\n",
    "* When the state-space and/or action-space become large, tabular solutions are impossible to construct.\n",
    "* In this type of scenario, we can potentially approximate solutions, but which in return can be applied effectively to much larger problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.1 Dynamic Programming\n",
    "* Pros: Mathematically sound solution framework.\n",
    "* Cons: Require a complete accurate model of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming--\n",
    "* The environment is a finite Markov Decision Process. That is, its state, action and reward sets, $S, A, R$ are finite, and its dynamics are given by a set of probabilities $p(S', r | s, a)$, for all $s\\in S, a\\in A(s), r\\in R, s'\\in S$ including the terminal state.\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What about DP solution for continue state & action spaces?\n",
    "* A common way of obtaining approximate solutions for tasks with continuous states and actions is to quantize the state and action spaces and then apply finite-state DP methods. (Chapter 9 in textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Finite state DP\n",
    "* We can easily obtain optimal policies once we have found the optimal value functions, $v_*$, or $q_*$, which satisfy the Bellman optimality equations, for all $s\\in S, a\\in A(s), s'\\in S$:\n",
    "![DP-v+q](figs/DP-v+q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy Evaluation, for estimating $V \\approx v_\\pi$\n",
    "![DP-policy-eval](figs/DP-policy-eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy Improvements\n",
    "* It is a natural extension to consider changes at all states, selecting at each state the action that appears best according to $q_\\pi (s,a)$.\n",
    "* In other words, to consider the new greedy policy, $\\pi'$, given by:\n",
    "![DP-policy-improv](figs/DP-improv-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Policy Iteration\n",
    "* Once a policy, $\\pi$ has been improved using $v_\\pi$ to yield a better policy, $\\pi'$, we can then compute $v_{\\pi'}$, and improve it again to yield an even better $\\pi''$.\n",
    "![DP-iter](figs/DP-iter-01.png)\n",
    "\n",
    "Here, $E$ on top of the arrows refer to a policy evaluation, and $I$ denotes a policy improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![DP-policy-iter](figs/DP-policy-iter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Drawback of \"Policy Iteration\"\n",
    "* Each of its iterations involves policy evaluation, which may itself be a longer iterative computation requiring multiple sweeps through the state set.\n",
    "* If policy evaluation is done iteratively, then convergence exactly to $v_\\pi$ occurs only in the limit. Should we wait for exact convergence, or can we stop short of that?\n",
    "    * We can stop in just one sweep (i.e., one update per state). This modified/reduced approach is called \"Value Iteration\".\n",
    "* Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement.\n",
    "    * Faster convergence can be achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Value Iteration\n",
    "\n",
    "![DP-val-iter](figs/DP-val-iter.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 Monte Carlo Methods\n",
    "* Pros: It does not require a model, and are conceptually simple.\n",
    "* Cons: The methods are not well suited for step-by-step incremental computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 Monte Carlo Methods\n",
    "* Estimating $V_\\pi(s)$, value of a state $s$ under policy $\\pi$ requires dealing with a set of episodes obtained by following $\\pi$ that are passing through $s$.\n",
    "* Each occurrence of state $s$ in an episode is called a visit to $s$. And, of course, $s$ may be visited multiple times in the same episode, that raises two MC methods:\n",
    "    * First visit MC method: estimates $v_\\pi(s)$ as the average of the returns following first visits to $s$.\n",
    "    * Every visit MC method: estimates $v_\\pi(s)$ by averaging the returns following all visits to $s$.\n",
    "* Both first visit MC and every visit MC converge to $v_\\pi(s)$ as the number of visists (or first visits) to $s$ goes to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![MC-pred](figs/MC-pred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating action values from $V$\n",
    "* With a model, state values (i.e., $v(s)$) alone are sufficient to determine a policy;\n",
    "    * One simply looks ahead one step and chooses whichever action leads to the best combinatio of reward and next step as we did in DP.\n",
    "* Without a model, state values alone are not sufficient.\n",
    "    * one must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for MC methods is to estimate $q_*$.\n",
    "    * The policy evaluation problem for action values is to estimate $q_\\pi(s, a)$, the expected return when starting in state $s$, taking action $$a, and thereafter following policy $\\pi$.\n",
    "    * The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a stateâ€“action pair rather than to a state.\n",
    "    * A state-action pair ($s$, $a$) is said to be visited in an episode if ever the state $s$ is visited and action $a$ is taken in it.\n",
    "    * Only complication is that many state-action pairs may never be visited. (Bummer, isn't it?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating action values from $V$\n",
    "* Without a model scenario:\n",
    "    * If $\\pi$ is a deterministic policy, then in following $\\pi$ one will observe returns only for one of the actions from each state.\n",
    "    * With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience.\n",
    "    * This is a serious problem.\n",
    "    * To mitigate the issue, we must assure continual exploration.\n",
    "    * One way to do this is by specifying that the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start.\n",
    "    * This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.\n",
    "    * We call this the assumption of **exploring starts**. However, this might not be ideal in practice. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Monte Carlo Exploring Start algorithm to estimate $\\pi$\n",
    "![MC-ES](figs/MC-ES.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Please do exercise 5.4 from book.\n",
    "![exercise5.4](figs/exercise-5_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MC without Exploring starts\n",
    "* How to avoid the unlikely assumption of exploring starts?\n",
    "* On policy control methods:\n",
    "    * attempt to evaluate or improve the policy that is used to make decisions.\n",
    "* Off policy control methods:\n",
    "    * attempt to evaluate or improve a policy different from that used to generate the data/episodes.\n",
    "* Note: MC ES is an on-policy method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## On-policy control methods\n",
    "* Here policy is generally *soft*, i.e., $\\pi(a|s) > 0$ for all $s\\in S, a\\in A(s)$, but gradually shifted closer and closer to a deterministic optimal policy.\n",
    "* Most on-policy methods use $\\epsilon$-greedy policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probablity $\\epsilon$ they instead select an action at random.\n",
    "* That means:\n",
    "    * All non-greedy actions are given the minimal probablity of selection, $\\dfrac{\\epsilon}{|A(s)|}$.\n",
    "    * Remaining bulk of the probability, $1 - \\epsilon + \\dfrac{\\epsilon}{|A(s)|}$, is given to the greedy action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### On policy first visit MC\n",
    "![On-policy-MC](figs/On-policy-MC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Off-policy prediction\n",
    "-Later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.3 Temporal Difference Learning\n",
    "* Pros: It also does not require a model, and are fully incremental.\n",
    "* Cons: The methods are complex to analyze.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Idea of Temporal difference learning to obtain $V$\n",
    "![TD-01](figs/TD-01.png)\n",
    "* Here, $\\alpha$ is a small positive fraction, called the step-size parameter, that influences the rate of learning.\n",
    "* This update rule so called because its changes are based on a difference $V(S_{t+1}) - V(S_t)$, between estimates at two successive times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to combine the tabular solution methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Approximate Solution Methods\n",
    "* Problems with large state spaces (and/or large action spaces), we can not expect to find an optimal policy or the optimal value function even in the limit of inifinite time and data.\n",
    "    * Goal is to find a good approximate solution using limited computational resources.\n",
    "    * How can experience with a limited subset of the state space be usefully **generalized** to produce a good approximation over a much larger subset?\n",
    "    * The idea of **function approximation**.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "rise": {
   "enable_chalkboard": true,
   "font-size": "80%",
   "height": 768,
   "margin": 0.25,
   "overlay": "<div class='myheader'><h2>ML-Lab@CU-Denver</h2></div>",
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky",
   "transition": "zoom",
   "width": 1024
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
